{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74678/587072343.py:2: FutureWarning: Could not cast to float32, falling back to object. This behavior is deprecated. In a future version, when a dtype is passed to 'DataFrame', either all columns will be cast to that dtype, or a TypeError will be raised\n",
      "  iemocap_dataset = IemocapDataset('/home/dstratton/IEMOCAP_full_release')\n"
     ]
    }
   ],
   "source": [
    "from torchemotion.datasets.IemocapDataset import *\n",
    "iemocap_dataset = IemocapDataset('/home/dstratton/IEMOCAP_full_release')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "trans = iemocap_dataset.df['transcription']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('/home/dstratton/IEMOCAP_full_release/Session*/dialog/transcriptions/*.txt')\n",
    "t = get_transcription_list(files[0])\n",
    "dialog = \" \".join(t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(iemocap_dataset.df[\"end\"] - iemocap_dataset.df[\"start\"])\n",
    "print(sum(iemocap_dataset.df[\"end\"] - iemocap_dataset.df[\"start\"] > 10))\n",
    "# remove super long clips\n",
    "iemocap_dataset.df = iemocap_dataset.df[iemocap_dataset.df[\"end\"] - iemocap_dataset.df[\"start\"] < 10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# todo: map labels to be in range [0, 4), since they aren't right now. maybe thats why nll loss is sad\n",
    "mapping = dict([(y,x) for x,y in enumerate(sorted(set(iemocap_dataset.df[\"emotion\"] )))])\n",
    "iemocap_dataset.df[\"emotion\"] = iemocap_dataset.df[\"emotion\"].apply(lambda x: mapping[x])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# splitting into sets\n",
    "import numpy as np, torch\n",
    "# https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset\n",
    "# todo: i could do this if i can rand seed it torch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>)\n",
    "np.random.seed(0)\n",
    "indices = np.arange(0, len(iemocap_dataset))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:int(len(iemocap_dataset)*.8)]\n",
    "val_indices = indices[int(len(iemocap_dataset)*.8):int(len(iemocap_dataset)*.9)]\n",
    "test_indices = indices[int(len(iemocap_dataset)*.9):]\n",
    "# train_set = torch.utils.data.Subset(iemocap_dataset, train_indices)\n",
    "# val_set = torch.utils.data.Subset(iemocap_dataset, val_indices)\n",
    "# test_set = torch.utils.data.Subset(iemocap_dataset, test_indices)\n",
    "# todo: may be good to balance or at least check the balance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import datasets\n",
    "\n",
    "data_dict = {}\n",
    "for split_name, indices in {\"train\": train_indices, \"val\": val_indices, \"test\": test_indices}.items():\n",
    "    ind = 0\n",
    "    df = pd.DataFrame(iemocap_dataset.df[\"emotion\"].iloc[indices]).reset_index()\n",
    "    # text\n",
    "    df['text'] = iemocap_dataset.df[\"transcription\"]\n",
    "    # audio\n",
    "    # df['audio'] = None\n",
    "    # for file_name in iemocap_dataset.df[\"file\"].iloc[indices]:\n",
    "    #     actual_file_name = '/home/dstratton/IEMOCAP_full_release/' + file_name\n",
    "    #     s, r = librosa.load(actual_file_name, sr=16000)\n",
    "    #     df['audio'][ind] = {'array': s, 'sampling_rate': r}\n",
    "    #     ind += 1\n",
    "    data_dict[split_name] = datasets.Dataset.from_pandas(df)\n",
    "\n",
    "data = datasets.DatasetDict(data_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# text\n",
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4424 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95792ac0e27d4286a6fe000ce35cc2e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/553 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef6da1322afa4b36a766cb8fc66c4be1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/554 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e18f24d65844162b540578847557404"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "data = data.map(tokenize_function)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4424 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77af76c8276a49a0a46bc08275fa83e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/553 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a7d95d7bfdc47b4a73fff86837ca3cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/554 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90d98660a409404da1b5caf4fb77d18a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_vals(example):\n",
    "    example[\"labels\"] = int(example[\"emotion\"])\n",
    "    return example\n",
    "data = data.map(map_vals, remove_columns=['emotion', 'index'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_q.weight', 'project_q.bias', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_hid.bias', 'quantizer.codevectors']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.weight', 'classifier.bias', 'projector.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# audio\n",
    "# base models\n",
    "import transformers\n",
    "# https://huggingface.co/facebook/wav2vec2-base\n",
    "# 16 kHz sampling rate in original\n",
    "processor = transformers.Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\", use_fast=True)\n",
    "model = transformers.AutoModelForAudioClassification.from_pretrained(\"facebook/wav2vec2-base\", num_labels=4).cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/datasets/audio_process.html\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"labels\"] = int(batch[\"emotion\"])\n",
    "    return batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4129 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b38ae9af88a49809efca9d4db9cc65b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/516 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "757c125af89a4ee28407b4a20f61ce1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/517 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49bde63a8468431bae9243f8d6a1b322"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.map(prepare_dataset,\n",
    "                batch_size=1,\n",
    "                remove_columns=['index'],\n",
    "                # num_proc=4\n",
    "                )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mderekdstratton\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=iemocap\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "%env WANDB_PROJECT=iemocap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"iemocap-output-text\",\n",
    "    # group_by_length=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=2,\n",
    "    report_to=\"wandb\",\n",
    "    num_train_epochs=6,\n",
    "    # learning_rate=1e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # save_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    # data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['val'],\n",
    "    # feature_extractor=processor.feature_extractor\n",
    "\n",
    "    # tokenizer=processor,\n",
    "    # tokenizer=tokenizer\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 4424\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13272\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mderekdstratton\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/derekdstratton/huggingface/runs/136gwu3f\" target=\"_blank\">iemocap-output-text</a></strong> to <a href=\"https://wandb.ai/derekdstratton/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='13272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/13272 : < :, Epoch 0.00/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=13272, training_loss=1.3895796096921327, metrics={'train_runtime': 1796.9082, 'train_samples_per_second': 14.772, 'train_steps_per_second': 7.386, 'total_flos': 6984145266868224.0, 'train_loss': 1.3895796096921327, 'epoch': 6.0})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 554\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/139 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'test_loss': 2.358397960662842,\n 'test_accuracy': 0.2743682310469314,\n 'test_runtime': 8.6034,\n 'test_samples_per_second': 64.393,\n 'test_steps_per_second': 16.156}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(data[\"test\"])\n",
    "\n",
    "# test_accuracy is correct, predictions for each class, you can take argmax\n",
    "predictions.metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 1.3449666500091553,\n 'eval_accuracy': 0.7732558139534884,\n 'eval_runtime': 18.9337,\n 'eval_samples_per_second': 27.253,\n 'eval_steps_per_second': 6.813,\n 'epoch': 6.0}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}