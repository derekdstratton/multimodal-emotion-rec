{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: fill this file out, i did a bunch of stuff lazily in the audio file earlier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# https://github.com/Samarth-Tripathi/IEMOCAP-Emotion-Detection/blob/master/code/python_files/helper.py\n",
    "def get_transcriptions(path_to_transcriptions):\n",
    "    f = open(path_to_transcriptions, 'r').read()\n",
    "    f = np.array(f.split('\\n'))\n",
    "    transcription = {}\n",
    "    for i in range(len(f) - 1):\n",
    "        g = f[i]\n",
    "        i1 = g.find(': ')\n",
    "        i0 = g.find(' [')\n",
    "        ind_id = g[:i0]\n",
    "        ind_ts = g[i1+2:]\n",
    "        transcription[ind_id] = ind_ts\n",
    "    return transcription"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "path_to_transcriptions = '/home/dstratton/IEMOCAP_full_release/Session1/dialog/transcriptions/Ses01F_impro01.txt'\n",
    "\n",
    "tra = get_transcriptions(path_to_transcriptions)\n",
    "# i think you can just glob all txt files from here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85263/587072343.py:2: FutureWarning: Could not cast to float32, falling back to object. This behavior is deprecated. In a future version, when a dtype is passed to 'DataFrame', either all columns will be cast to that dtype, or a TypeError will be raised\n",
      "  iemocap_dataset = IemocapDataset('/home/dstratton/IEMOCAP_full_release')\n"
     ]
    }
   ],
   "source": [
    "from torchemotion.datasets.IemocapDataset import *\n",
    "iemocap_dataset = IemocapDataset('/home/dstratton/IEMOCAP_full_release')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trans = iemocap_dataset.df['transcription']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(iemocap_dataset.df[\"end\"] - iemocap_dataset.df[\"start\"])\n",
    "print(sum(iemocap_dataset.df[\"end\"] - iemocap_dataset.df[\"start\"] > 10))\n",
    "# remove super long clips\n",
    "iemocap_dataset.df = iemocap_dataset.df[iemocap_dataset.df[\"end\"] - iemocap_dataset.df[\"start\"] < 10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('/home/dstratton/IEMOCAP_full_release/Session*/dialog/transcriptions/*.txt')\n",
    "t = get_transcription_list(files[0])\n",
    "dialog = \" \".join(t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# todo: map labels to be in range [0, 4), since they aren't right now. maybe thats why nll loss is sad\n",
    "mapping = dict([(y,x) for x,y in enumerate(sorted(set(iemocap_dataset.df[\"emotion\"] )))])\n",
    "iemocap_dataset.df[\"emotion\"] = iemocap_dataset.df[\"emotion\"].apply(lambda x: mapping[x])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# splitting into sets\n",
    "import numpy as np, torch\n",
    "# https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset\n",
    "# todo: i could do this if i can rand seed it torch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>)\n",
    "np.random.seed(0)\n",
    "indices = np.arange(0, len(iemocap_dataset))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:int(len(iemocap_dataset)*.8)]\n",
    "val_indices = indices[int(len(iemocap_dataset)*.8):int(len(iemocap_dataset)*.9)]\n",
    "test_indices = indices[int(len(iemocap_dataset)*.9):]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import datasets\n",
    "\n",
    "data_dict = {}\n",
    "for split_name, indices in {\"train\": train_indices, \"val\": val_indices, \"test\": test_indices}.items():\n",
    "    ind = 0\n",
    "    df = pd.DataFrame(iemocap_dataset.df[\"emotion\"].iloc[indices]).reset_index()\n",
    "    # text\n",
    "    df['text'] = iemocap_dataset.df[\"transcription\"]\n",
    "    # audio\n",
    "    # df['audio'] = None\n",
    "    # for file_name in iemocap_dataset.df[\"file\"].iloc[indices]:\n",
    "    #     actual_file_name = '/home/dstratton/IEMOCAP_full_release/' + file_name\n",
    "    #     s, r = librosa.load(actual_file_name, sr=16000)\n",
    "    #     df['audio'][ind] = {'array': s, 'sampling_rate': r}\n",
    "    #     ind += 1\n",
    "    data_dict[split_name] = datasets.Dataset.from_pandas(df)\n",
    "\n",
    "data = datasets.DatasetDict(data_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# text\n",
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4424 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0cf9b228a0646ec9109aab7d4e0555b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/553 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ba7cd4bb8964c0f826193dfcea0a56d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/554 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4d95cc0180c4343a69031b41cb484b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "data = data.map(tokenize_function)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4424 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7d6586b243d4fd9b3b70c96db036f2b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/553 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc6a6c2376f24ba6a2d8641df7c597c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/554 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88e21f4ffc7a43419ba9b2e245277e37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_vals(example):\n",
    "    example[\"labels\"] = int(example[\"emotion\"])\n",
    "    return example\n",
    "data = data.map(map_vals, remove_columns=['emotion', 'index'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mderekdstratton\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=multimodal-emotion-rec\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "%env WANDB_PROJECT=multimodal-emotion-rec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\"iemocap-text-model\",\n",
    "                                  num_train_epochs=20,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  report_to=\"wandb\",\n",
    "                                  learning_rate=1e-5,\n",
    "                                  per_device_train_batch_size=2)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"val\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 4424\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 22120\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: wandb version 0.12.12 is available!  To upgrade, please run:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/derekdstratton/multimodal-emotion-rec/runs/1pdwct18\" target=\"_blank\">iemocap-text-model</a></strong> to <a href=\"https://wandb.ai/derekdstratton/multimodal-emotion-rec\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='22120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/22120 : < :, Epoch 0.00/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-1106\n",
      "Configuration saved in iemocap-text-model/checkpoint-1106/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-1106/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-2212\n",
      "Configuration saved in iemocap-text-model/checkpoint-2212/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-2212/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-3318\n",
      "Configuration saved in iemocap-text-model/checkpoint-3318/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-3318/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-4424\n",
      "Configuration saved in iemocap-text-model/checkpoint-4424/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-4424/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-5530\n",
      "Configuration saved in iemocap-text-model/checkpoint-5530/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-5530/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-6636\n",
      "Configuration saved in iemocap-text-model/checkpoint-6636/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-6636/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-7742\n",
      "Configuration saved in iemocap-text-model/checkpoint-7742/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-7742/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-8848\n",
      "Configuration saved in iemocap-text-model/checkpoint-8848/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-8848/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-9954\n",
      "Configuration saved in iemocap-text-model/checkpoint-9954/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-9954/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-11060\n",
      "Configuration saved in iemocap-text-model/checkpoint-11060/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-11060/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-12166\n",
      "Configuration saved in iemocap-text-model/checkpoint-12166/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-12166/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-13272\n",
      "Configuration saved in iemocap-text-model/checkpoint-13272/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-13272/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-14378\n",
      "Configuration saved in iemocap-text-model/checkpoint-14378/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-14378/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-15484\n",
      "Configuration saved in iemocap-text-model/checkpoint-15484/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-15484/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-16590\n",
      "Configuration saved in iemocap-text-model/checkpoint-16590/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-16590/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-17696\n",
      "Configuration saved in iemocap-text-model/checkpoint-17696/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-17696/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-18802\n",
      "Configuration saved in iemocap-text-model/checkpoint-18802/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-18802/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-19908\n",
      "Configuration saved in iemocap-text-model/checkpoint-19908/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-19908/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-21014\n",
      "Configuration saved in iemocap-text-model/checkpoint-21014/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-21014/pytorch_model.bin\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 553\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to iemocap-text-model/checkpoint-22120\n",
      "Configuration saved in iemocap-text-model/checkpoint-22120/config.json\n",
      "Model weights saved in iemocap-text-model/checkpoint-22120/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 85399... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84e5fee4d26243939c1f5b971ac3b920"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▅█▃▁▂▂▃▂▁▂▁▂▂▂▁▁▂▂▂▂</td></tr><tr><td>eval/loss</td><td>▁▁▁▂▂▃▄▅▆▆▇▇▇▇██████</td></tr><tr><td>eval/runtime</td><td>▁▅▄▄▃▃▃▄▄▂█▄█▄▄▄▄█▄▄</td></tr><tr><td>eval/samples_per_second</td><td>█▄▅▅▆▆▆▅▅▇▁▅▁▅▅▅▅▁▅▅</td></tr><tr><td>eval/steps_per_second</td><td>█▄▅▅▆▆▆▅▅▇▁▅▁▅▅▅▅▁▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█████▇▇▆▆▄▃▄▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.27486</td></tr><tr><td>eval/loss</td><td>6.31591</td></tr><tr><td>eval/runtime</td><td>5.9487</td></tr><tr><td>eval/samples_per_second</td><td>92.962</td></tr><tr><td>eval/steps_per_second</td><td>5.884</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>22120</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2582</td></tr><tr><td>train/total_flos</td><td>2.328048422289408e+16</td></tr><tr><td>train/train_loss</td><td>0.58769</td></tr><tr><td>train/train_runtime</td><td>4171.5153</td></tr><tr><td>train/train_samples_per_second</td><td>21.211</td></tr><tr><td>train/train_steps_per_second</td><td>5.303</td></tr></table>\n</div></div>\nSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">iemocap-text-model</strong>: <a href=\"https://wandb.ai/derekdstratton/multimodal-emotion-rec/runs/1pdwct18\" target=\"_blank\">https://wandb.ai/derekdstratton/multimodal-emotion-rec/runs/1pdwct18</a><br/>\nFind logs at: <code>./wandb/run-20220406_113537-1pdwct18/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 554\n",
      "  Batch size = 16\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/35 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'test_loss': 6.275918483734131,\n 'test_accuracy': 0.26173285198555957,\n 'test_runtime': 5.8914,\n 'test_samples_per_second': 94.036,\n 'test_steps_per_second': 5.941}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(data[\"test\"])\n",
    "\n",
    "# test_accuracy is correct, predictions for each class, you can take argmax\n",
    "predictions.metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}