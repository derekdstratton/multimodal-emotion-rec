{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# pretrained\n",
    "# import transformers\n",
    "# processor = transformers.Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\", use_fast=True)\n",
    "# model = transformers.AutoModelForAudioClassification.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\").cuda()\n",
    "# pred = transformers.pipeline(\"audio-classification\", model=model, tokenizer=processor.tokenizer, feature_extractor=processor.feature_extractor, device=0, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# pretrained model trying\n",
    "# import librosa\n",
    "# import torch\n",
    "# import glob\n",
    "# audios = glob.glob(\"cmumosi/Raw/Audio/WAV_16000/Segmented/*.wav\")[:100]\n",
    "# outs = []\n",
    "#\n",
    "# for a in audios:\n",
    "#     try:\n",
    "#         speech, rate = librosa.load(a, sr=16000)\n",
    "#         speech = torch.Tensor(speech).cuda()\n",
    "#         # https://www.analyticsvidhya.com/blog/2021/02/hugging-face-introduces-the-first-automatic-speech-recognition-model-wav2vec2/\n",
    "#         input_values = processor(speech, sampling_rate = 16000, return_tensors = 'pt').input_values.cuda()\n",
    "#         #features_extracted = processor.feature_extractor(speech)\n",
    "#         logits = model(input_values).logits\n",
    "#         # https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/wav2vec2#transformers.Wav2Vec2Model\n",
    "#         # emotions = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
    "#         # predicted_ids = torch.argmax(logits, dim=-1)\n",
    "#         # transcription = emotions[predicted_ids.item()]\n",
    "#         # outs.append(transcription)\n",
    "#     except RuntimeError:\n",
    "#         print(\"rip\") # cuda out of memory for too big\n",
    "#     # clean it up tho\n",
    "#     del speech\n",
    "#     torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/transformers/configuration_utils.py:348: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_hid.weight', 'project_q.bias', 'quantizer.codevectors', 'project_q.weight', 'quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.weight', 'projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model2 = transformers.AutoModelForAudioClassification.from_pretrained(\"facebook/wav2vec2-base\", num_labels=2).cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.weight', 'classifier.weight', 'projector.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# wav2vec\n",
    "import transformers\n",
    "# https://huggingface.co/facebook/wav2vec2-base\n",
    "# 16 kHz sampling rate in original\n",
    "processor = transformers.Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\", use_fast=True)\n",
    "model = transformers.AutoModelForAudioClassification.from_pretrained(\"facebook/wav2vec2-base\", num_labels=2).cuda()\n",
    "# pred = transformers.pipeline(\"audio-classification\", model=model, tokenizer=processor.tokenizer, feature_extractor=processor.feature_extractor, device=0, return_all_scores=True)\n",
    "\n",
    "# trying out\n",
    "# https://huggingface.co/docs/transformers/model_doc/hubert\n",
    "# https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertForSequenceClassification\n",
    "\n",
    "# feature_extractor = transformers.Wav2Vec2FeatureExtractor.from_pretrained(\"ntu-spml/distilhubert\")\n",
    "# model = transformers.HubertForSequenceClassification.from_pretrained(\"ntu-spml/distilhubert\", num_labels=2).cuda()\n",
    "# pred = transformers.pipeline(\"audio-classification\", model=model, tokenizer=processor.tokenizer, feature_extractor=processor.feature_extractor, device=0, return_all_scores=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\") # same as processor's feature extractor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[91m\u001B[1m[2022-02-18 21:48:11.768] | Error   | \u001B[0mcmumosi/CMU_MOSI_TimestampedWordVectors.csd file already exists ...\n",
      "High-level features have been downloaded previously.\n",
      "\u001B[91m\u001B[1m[2022-02-18 21:48:11.768] | Error   | \u001B[0mcmumosi/CMU_MOSI_TimestampedWords.csd file already exists ...\n",
      "Raw data have been downloaded previously.\n",
      "\u001B[91m\u001B[1m[2022-02-18 21:48:11.768] | Error   | \u001B[0mcmumosi/CMU_MOSI_Opinion_Labels.csd file already exists ...\n",
      "Labels have been downloaded previously.\n",
      "\u001B[92m\u001B[1m[2022-02-18 21:48:11.768] | Success | \u001B[0mComputational sequence read from file cmumosi/CMU_MOSI_Opinion_Labels.csd ...\n",
      "\u001B[94m\u001B[1m[2022-02-18 21:48:11.781] | Status  | \u001B[0mChecking the integrity of the <Opinion Segment Labels> computational sequence ...\n",
      "\u001B[94m\u001B[1m[2022-02-18 21:48:11.781] | Status  | \u001B[0mChecking the format of the data in <Opinion Segment Labels> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92m\u001B[1m[2022-02-18 21:48:11.814] | Success | \u001B[0m<Opinion Segment Labels> computational sequence data in correct format.\n",
      "\u001B[94m\u001B[1m[2022-02-18 21:48:11.814] | Status  | \u001B[0mChecking the format of the metadata in <Opinion Segment Labels> computational sequence ...\n",
      "\u001B[93m\u001B[1m[2022-02-18 21:48:11.814] | Warning | \u001B[0m<Opinion Segment Labels> computational sequence does not have all the required metadata ... continuing \n",
      "\u001B[92m\u001B[1m[2022-02-18 21:48:11.814] | Success | \u001B[0mDataset initialized successfully ... \n",
      "\u001B[94m\u001B[1m[2022-02-18 21:48:11.814] | Status  | \u001B[0mUnify was called ...\n",
      "\u001B[92m\u001B[1m[2022-02-18 21:48:11.814] | Success | \u001B[0mUnify completed ...\n",
      "\u001B[94m\u001B[1m[2022-02-18 21:48:11.814] | Status  | \u001B[0mPre-alignment based on <CMU_MOSI_Opinion_Labels> computational sequence started ...\n",
      "\u001B[94m\u001B[1m[2022-02-18 21:48:11.814] | Status  | \u001B[0mAlignment starting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|          | 0/93 [00:00<?, ? Computational Sequence Entries/s]\n",
      "  0%|          | 0/13 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 03bSnISJMiM:   0%|          | 0/13 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 0h-zjBukYpk:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 1DmNV9C1hbY:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 1iG0909rllw:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/63 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 2WGyTLYerpo:   0%|          | 0/63 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 2iD-tVS8NPw:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 5W7Z1C_fDaE:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 6Egk_28TtTM:   0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:   9%|▊         | 8/93 [00:00<00:01, 74.67 Computational Sequence Entries/s]\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 6_0THN4chvY:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/19 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 73jzhE8R1TQ:   0%|          | 0/19 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/39 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 7JsX8y1ysxY:   0%|          | 0/39 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/23 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 8OtFthrtaJM:   0%|          | 0/23 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 8d-gEyoeBzc:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/26 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 8qrpnFRGt2A:   0%|          | 0/26 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 9J25DZhivz8:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 9T9Hf74oK10:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  17%|█▋        | 16/93 [00:00<00:01, 70.78 Computational Sequence Entries/s]\n",
      "  0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 9c67fiY0wGQ:   0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/33 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 9qR7uwkblbs:   0%|          | 0/33 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Af8D0E4ZXaw:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning BI97DNYfe5I:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning BXuRRbG0Ugk:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Bfr499ggo-0:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning BioHAh1qJAQ:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/26 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning BvYR0L6f2Ig:   0%|          | 0/26 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  26%|██▌       | 24/93 [00:00<00:00, 69.52 Computational Sequence Entries/s]\n",
      "  0%|          | 0/44 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Ci-AH39fi3Y:   0%|          | 0/44 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Clx4VXItLTE:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Dg_0XKD0Mf4:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/21 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning G-xst2euQUc:   0%|          | 0/21 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/29 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning G6GlGvlkxAQ:   0%|          | 0/29 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning GWuJjcEuzt8:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/34 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning HEsqda8_d0Q:   0%|          | 0/34 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  33%|███▎      | 31/93 [00:00<00:00, 68.52 Computational Sequence Entries/s]\n",
      "  0%|          | 0/39 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning I5y0__X72p0:   0%|          | 0/39 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Iu2PFX3z_1s:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning IumbAb8q2dM:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/20 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Jkswaaud0hk:   0%|          | 0/20 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/29 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning LSi-o-IrDMs:   0%|          | 0/29 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning MLal-t_vJPM:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/13 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Njd1F0vZSm4:   0%|          | 0/13 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/32 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Nzq88NnDkEk:   0%|          | 0/32 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  42%|████▏     | 39/93 [00:00<00:00, 70.56 Computational Sequence Entries/s]\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning OQvJTdtJ2H4:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning OtBXNcAL_lE:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Oz06ZWiO20M:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/13 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning POKffnXeBds:   0%|          | 0/13 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning PZ-lDQFboO8:   0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning QN9ZIUWUXsY:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Qr1Ca94K55A:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Sqr0AcuoNnk:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning TvyZBvOMOTc:   0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/17 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning VCslbP0mgZI:   0%|          | 0/17 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  53%|█████▎    | 49/93 [00:00<00:00, 79.93 Computational Sequence Entries/s]\n",
      "  0%|          | 0/55 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning VbQk4H8hgr0:   0%|          | 0/55 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/9 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Vj1wYRQjB-o:   0%|          | 0/9 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                  \u001B[A\n",
      "  0%|          | 0/32 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning W8NXH0Djyww:   0%|          | 0/32 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning WKA5OygbEKI:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/11 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning X3j2zQgwYgE:   0%|          | 0/11 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/9 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning ZAIRrfG22O0:   0%|          | 0/9 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                  \u001B[A\n",
      "  0%|          | 0/34 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning ZUXBRvtny7o:   0%|          | 0/34 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/28 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning _dI--eQ6qVU:   0%|          | 0/28 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning aiEXnCPZubE:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  62%|██████▏   | 58/93 [00:00<00:00, 77.80 Computational Sequence Entries/s]\n",
      "  0%|          | 0/21 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning atnd_PF-Lbs:   0%|          | 0/21 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/34 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning bOL9jKpeJRs:   0%|          | 0/34 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning bvLlb-M3UXU:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning c5xsKMxpXnc:   0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/33 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning c7UH_rxdZv4:   0%|          | 0/33 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning cM3Yna7AavY:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning cW1FSBF59ik:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/29 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning cXypl4FnoZo:   0%|          | 0/29 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  71%|███████   | 66/93 [00:00<00:00, 77.10 Computational Sequence Entries/s]\n",
      "  0%|          | 0/19 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning d3_k5Xpfmik:   0%|          | 0/19 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/43 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning d6hH302o4v8:   0%|          | 0/43 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning dq3Nf_lMPnE:   0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/19 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning etzxEpPuc6I:   0%|          | 0/19 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning f9O3YtZ2VfI:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning f_pcplsH_V0:   0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning fvVhgmXxadc:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning iiK8YX8oH1E:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/27 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning jUzDDGyPkXU:   0%|          | 0/27 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  81%|████████  | 75/93 [00:00<00:00, 78.79 Computational Sequence Entries/s]\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning k5Y_838nuGo:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning lXPQBPVc5Cw:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/10 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning nbWiPyCm4g0:   0%|          | 0/10 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning nzpVDcQ0ywM:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning ob23OKe5a9Q:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning pLTX3ipuDJI:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/21 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning phBUpBr1hSo:   0%|          | 0/21 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning rnaNMUZpvvg:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning tIrG4oNLFzE:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  90%|█████████ | 84/93 [00:01<00:00, 81.65 Computational Sequence Entries/s]\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning tStelxIAHjw:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/20 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning tmZoasNr4rU:   0%|          | 0/20 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning v0zCBqDeKcE:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning vvZ4IcEtiZc:   0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning vyB00TXsimI:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning wMbj6ajWbic:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning yDtzw_Y-7RU:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/23 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning yvsjCA6Y5Fc:   0%|          | 0/23 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/35 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning zhpQhgha_KU:   0%|          | 0/35 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92m\u001B[1m[2022-02-18 21:48:13.017] | Success | \u001B[0mAlignment to <CMU_MOSI_Opinion_Labels> complete.\n",
      "\u001B[94m\u001B[1m[2022-02-18 21:48:13.017] | Status  | \u001B[0mReplacing dataset content with aligned computational sequences\n",
      "\u001B[92m\u001B[1m[2022-02-18 21:48:13.017] | Success | \u001B[0mInitialized empty <CMU_MOSI_Opinion_Labels> computational sequence.\n",
      "\u001B[94m\u001B[1m[2022-02-18 21:48:13.017] | Status  | \u001B[0mChecking the format of the data in <CMU_MOSI_Opinion_Labels> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/PycharmProjects/InterpretableMultimodal/utilss.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['file'][ind] = actual_file_name\n",
      "/home/dstratton/PycharmProjects/InterpretableMultimodal/utilss.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['audio'][ind] = {'array': s, 'sampling_rate': r, 'path': actual_file_name}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92m\u001B[1m[2022-02-18 21:48:13.020] | Success | \u001B[0m<CMU_MOSI_Opinion_Labels> computational sequence data in correct format.\n",
      "\u001B[94m\u001B[1m[2022-02-18 21:48:13.020] | Status  | \u001B[0mChecking the format of the metadata in <CMU_MOSI_Opinion_Labels> computational sequence ...\n",
      "\u001B[93m\u001B[1m[2022-02-18 21:48:13.020] | Warning | \u001B[0m<CMU_MOSI_Opinion_Labels> computational sequence does not have all the required metadata ... continuing \n"
     ]
    }
   ],
   "source": [
    "# loading mosi into datasets dict\n",
    "# https://huggingface.co/blog/fine-tune-wav2vec2-english\n",
    "from utilss import load_dataset\n",
    "data = load_dataset([], load_audio=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_1.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_2.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_3.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_4.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_5.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_6.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_7.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_8.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_9.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_10.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_11.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_12.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_13.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_1.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_2.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_3.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_4.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_5.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_6.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_7.wav']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['file'][:20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/datasets/audio_process.html\n",
    "def prepare_dataset(batch):\n",
    "\n",
    "    # batch[\"input_values\"] = batch[\"audio\"]['array']\n",
    "\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "\n",
    "    # batch[\"input_values\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "\n",
    "    # this is an numpy array, shhould this be a tensor, and on the gpu?\n",
    "    # batch[\"input_values\"] = feature_extractor(batch[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_values[0].cuda()\n",
    "\n",
    "    # batch[\"input_values\"] = feature_extractor(batch[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "    # batch[\"input_values\"] = batch[\"input_values\"].cuda()\n",
    "\n",
    "    # batch[\"input_values\"] =\n",
    "\n",
    "    batch[\"labels\"] = batch[\"labels\"]\n",
    "    # with processor.as_target_processor():\n",
    "    #     batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1283 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e7e6e2ca75b45ec8c6115a882ef8124"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/229 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09970c22108743d2bee8db9664c4fdd0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/686 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc9ee0b0bd884d53a67992cda055f2ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols_to_remove = data.column_names['train']\n",
    "cols_to_remove.remove('labels') # don't remove labels\n",
    "data = data.map(prepare_dataset,\n",
    "                remove_columns=cols_to_remove,\n",
    "                # num_proc=4\n",
    "                )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# this is a thing in pytorch, but not on the GPU!!!\n",
    "# inputs = feature_extractor(data[\"train\"][0][\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_values[0].cuda()\n",
    "inputs = feature_extractor(data[\"train\"][0][\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "inputs.data['input_values'] = inputs.data['input_values'].cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "inputs2 = data[\"train\"][\"input_values\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_33658/1135863342.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# del inputs2['file']\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# del inputs2['audio']\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0minputs2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/transformers/models/hubert/modeling_hubert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001B[0m\n\u001B[1;32m   1208\u001B[0m         \u001B[0moutput_hidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muse_weighted_layer_sum\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1209\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1210\u001B[0;31m         outputs = self.hubert(\n\u001B[0m\u001B[1;32m   1211\u001B[0m             \u001B[0minput_values\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1212\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/transformers/models/hubert/modeling_hubert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1003\u001B[0m         \u001B[0mreturn_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreturn_dict\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mreturn_dict\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muse_return_dict\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1004\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1005\u001B[0;31m         \u001B[0mextract_features\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeature_extractor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_values\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1006\u001B[0m         \u001B[0mextract_features\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mextract_features\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1007\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/transformers/models/hubert/modeling_hubert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_values)\u001B[0m\n\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_values\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 314\u001B[0;31m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput_values\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    315\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    316\u001B[0m         \u001B[0;31m# make sure hidden_states require grad for gradient_checkpointing\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# del inputs2['file']\n",
    "# del inputs2['audio']\n",
    "model(**inputs2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1256, -0.1170]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mderekdstratton\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=lets_go\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=lets_go"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "lr_scheduler = AdafactorSchedule(optimizer)\n",
    "\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# TODO: maybe i need to resample the audio or do something with the\n",
    "# model? batch size of 1 is slightly less than ideal. 35 min 3 epoch\n",
    "# 20 min for 3 epochs on batch of 2. 4 is error i think\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"audio-output\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=2,\n",
    "  per_device_eval_batch_size=2,\n",
    "  report_to=\"wandb\",\n",
    "  num_train_epochs=6,\n",
    "  # num_train_epochs=30,\n",
    "    # todo: weight decay?\n",
    "    # weight_decay=\n",
    "  # fp16=True,\n",
    "  # gradient_checkpointing=True,\n",
    "  # save_steps=200,\n",
    "  # eval_steps=200,\n",
    "  # logging_steps=200,\n",
    "    # default is 5e-5\n",
    "  # learning_rate=5e-6,\n",
    "  learning_rate=None,\n",
    "\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  # save_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "  # weight_decay=0.005,\n",
    "  # warmup_steps=1000,\n",
    "  # save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "        optimizers=(optimizer, lr_scheduler),\n",
    "    # data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"val\"],\n",
    "    # tokenizer=feature_extractor,\n",
    "    tokenizer=processor\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "# make a callback to record training loss\n",
    "# https://stackoverflow.com/questions/67457480/how-to-get-the-accuracy-per-epoch-or-step-for-the-huggingface-transformers-train\n",
    "# https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/callback\n",
    "class CustomCallback(TrainerCallback):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # self._trainer = trainer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = deepcopy(control)\n",
    "            trainer.evaluate(eval_dataset=trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            # return control\n",
    "\n",
    "            # self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy\n",
    "\n",
    "# trainer.add_callback(CustomCallback())\n",
    "# trainer.add_callback(CustomCallback(trainer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1283\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1926\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mderekdstratton\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/derekdstratton/huggingface/runs/3d4kl5vt\" target=\"_blank\">audio-output</a></strong> to <a href=\"https://wandb.ai/derekdstratton/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1926' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1926 : < :, Epoch 0.00/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 229\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 229\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 229\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 229\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 229\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 229\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_186567/2254120986.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mtrain_results\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mwandb\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfinish\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 686\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='172' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/172 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'test_loss': 0.7078453898429871,\n 'test_accuracy': 0.4037900874635568,\n 'test_runtime': 28.0898,\n 'test_samples_per_second': 24.422,\n 'test_steps_per_second': 6.123}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(data[\"test\"])\n",
    "\n",
    "# test_accuracy is correct, predictions for each class, you can take argmax\n",
    "predictions.metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# data collator (https://huggingface.co/blog/fine-tune-wav2vec2-english)\n",
    "# todo: learn what this is! LOL\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "# data_collator = DataCollatorCTCWithPadding(processor=feature_extractor, padding=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "1    678\n0    605\ndtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_labels = pd.DataFrame(data['train']['labels'])\n",
    "train_labels.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "1    124\n0    105\ndtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels = pd.DataFrame(data['val']['labels'])\n",
    "val_labels.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "0    409\n1    277\ndtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = pd.DataFrame(data['test']['labels'])\n",
    "test_labels.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_features = pd.DataFrame(data['train']['input_values'][:5])\n",
    "# this ends up being like 5 rows, 160K cols lol"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import shap\n",
    "# explainer = shap.Explainer(pred, processor)\n",
    "# way too slow. any way of looking at it by segmenting off words?\n",
    "# shap_values = explainer([speech])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "OKAY\n",
    "\n",
    "so i can split up the raw audio by text alignments, and get embeddings from a pretrained transformer fine tuned. but how does that help? i'd still need a sequence classifier to train... at that point i might as well use the embeddings given, the advantage is i don't have to use a collaspe function i guess...\n",
    "\n",
    "Unless there's a more efficient way to use shap to remove a bunch of values (specifically all the values based on alignments). if that could be done that might work? but since the model is based on a sequence, not sure how that would even work.\n",
    "\n",
    "I don't think training a transformer from scratch on AUs/other features like this, aligned at the word level from a collapse function is a good idea. There's so much that can go wrong, I probably don't have enough data, I don't like collapsing all the data either if I can avoid it.\n",
    "\n",
    "https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/sentiment_analysis/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html?highlight=lstm\n",
    "If I can use Shap on an LSTM, I could fine tune a transformer to get embeddings, then train an LSTM with those embeddings as input. The transformer would be applied to audio at the word level, so theoretically I don't lose information the way I would with a \"collapse function\". So I don't necessarily know \"what\" the model is looking for ignoring (AU or pitch increase), but I know when. And that could be interesting. If anything I can try changing the LSTM to a transformer itself later, but for now LSTMs would just be easier to implement anyway.\n",
    "\n",
    "The problem is idk if there's even enough data for a deep learning LSTM in this dataset... "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}