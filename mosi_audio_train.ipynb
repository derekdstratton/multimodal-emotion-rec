{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_q.weight', 'project_q.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.weight', 'projector.bias', 'projector.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# wav2vec2\n",
    "import transformers\n",
    "# https://huggingface.co/facebook/wav2vec2-base\n",
    "# 16 kHz sampling rate in original\n",
    "processor = transformers.Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\", use_fast=True)\n",
    "model = transformers.AutoModelForAudioClassification.from_pretrained(\"facebook/wav2vec2-base\", num_labels=2).cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.weight', 'classifier.weight', 'projector.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# todo: why did i do this?\n",
    "model.freeze_feature_extractor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\") # same as processor's feature extractor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[91m\u001B[1m[2022-04-02 23:12:15.203] | Error   | \u001B[0mcmumosi/CMU_MOSI_TimestampedWordVectors.csd file already exists ...\n",
      "High-level features have been downloaded previously.\n",
      "\u001B[91m\u001B[1m[2022-04-02 23:12:15.203] | Error   | \u001B[0mcmumosi/CMU_MOSI_TimestampedWords.csd file already exists ...\n",
      "Raw data have been downloaded previously.\n",
      "\u001B[91m\u001B[1m[2022-04-02 23:12:15.203] | Error   | \u001B[0mcmumosi/CMU_MOSI_Opinion_Labels.csd file already exists ...\n",
      "Labels have been downloaded previously.\n",
      "\u001B[92m\u001B[1m[2022-04-02 23:12:15.204] | Success | \u001B[0mComputational sequence read from file cmumosi/CMU_MOSI_Opinion_Labels.csd ...\n",
      "\u001B[94m\u001B[1m[2022-04-02 23:12:15.210] | Status  | \u001B[0mChecking the integrity of the <Opinion Segment Labels> computational sequence ...\n",
      "\u001B[94m\u001B[1m[2022-04-02 23:12:15.210] | Status  | \u001B[0mChecking the format of the data in <Opinion Segment Labels> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92m\u001B[1m[2022-04-02 23:12:15.248] | Success | \u001B[0m<Opinion Segment Labels> computational sequence data in correct format.\n",
      "\u001B[94m\u001B[1m[2022-04-02 23:12:15.248] | Status  | \u001B[0mChecking the format of the metadata in <Opinion Segment Labels> computational sequence ...\n",
      "\u001B[93m\u001B[1m[2022-04-02 23:12:15.248] | Warning | \u001B[0m<Opinion Segment Labels> computational sequence does not have all the required metadata ... continuing \n",
      "\u001B[92m\u001B[1m[2022-04-02 23:12:15.248] | Success | \u001B[0mDataset initialized successfully ... \n",
      "\u001B[94m\u001B[1m[2022-04-02 23:12:15.248] | Status  | \u001B[0mUnify was called ...\n",
      "\u001B[92m\u001B[1m[2022-04-02 23:12:15.248] | Success | \u001B[0mUnify completed ...\n",
      "\u001B[94m\u001B[1m[2022-04-02 23:12:15.248] | Status  | \u001B[0mPre-alignment based on <CMU_MOSI_Opinion_Labels> computational sequence started ...\n",
      "\u001B[94m\u001B[1m[2022-04-02 23:12:15.248] | Status  | \u001B[0mAlignment starting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|          | 0/93 [00:00<?, ? Computational Sequence Entries/s]\n",
      "  0%|          | 0/13 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 03bSnISJMiM:   0%|          | 0/13 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 0h-zjBukYpk:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 1DmNV9C1hbY:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 1iG0909rllw:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/63 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 2WGyTLYerpo:   0%|          | 0/63 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 2iD-tVS8NPw:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:   6%|▋         | 6/93 [00:00<00:01, 57.97 Computational Sequence Entries/s]\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 5W7Z1C_fDaE:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 6Egk_28TtTM:   0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 6_0THN4chvY:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/19 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 73jzhE8R1TQ:   0%|          | 0/19 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/39 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 7JsX8y1ysxY:   0%|          | 0/39 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/23 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 8OtFthrtaJM:   0%|          | 0/23 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 8d-gEyoeBzc:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/26 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 8qrpnFRGt2A:   0%|          | 0/26 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  15%|█▌        | 14/93 [00:00<00:01, 69.61 Computational Sequence Entries/s]\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 9J25DZhivz8:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 9T9Hf74oK10:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 9c67fiY0wGQ:   0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/33 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning 9qR7uwkblbs:   0%|          | 0/33 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Af8D0E4ZXaw:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning BI97DNYfe5I:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning BXuRRbG0Ugk:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Bfr499ggo-0:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  24%|██▎       | 22/93 [00:00<00:00, 72.16 Computational Sequence Entries/s]\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning BioHAh1qJAQ:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/26 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning BvYR0L6f2Ig:   0%|          | 0/26 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/44 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Ci-AH39fi3Y:   0%|          | 0/44 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Clx4VXItLTE:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Dg_0XKD0Mf4:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/21 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning G-xst2euQUc:   0%|          | 0/21 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/29 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning G6GlGvlkxAQ:   0%|          | 0/29 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning GWuJjcEuzt8:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  32%|███▏      | 30/93 [00:00<00:00, 71.19 Computational Sequence Entries/s]\n",
      "  0%|          | 0/34 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning HEsqda8_d0Q:   0%|          | 0/34 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/39 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning I5y0__X72p0:   0%|          | 0/39 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Iu2PFX3z_1s:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning IumbAb8q2dM:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/20 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Jkswaaud0hk:   0%|          | 0/20 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/29 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning LSi-o-IrDMs:   0%|          | 0/29 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning MLal-t_vJPM:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/13 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Njd1F0vZSm4:   0%|          | 0/13 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  41%|████      | 38/93 [00:00<00:00, 70.76 Computational Sequence Entries/s]\n",
      "  0%|          | 0/32 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Nzq88NnDkEk:   0%|          | 0/32 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning OQvJTdtJ2H4:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning OtBXNcAL_lE:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Oz06ZWiO20M:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/13 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning POKffnXeBds:   0%|          | 0/13 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning PZ-lDQFboO8:   0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning QN9ZIUWUXsY:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Qr1Ca94K55A:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Sqr0AcuoNnk:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  51%|█████     | 47/93 [00:00<00:00, 76.63 Computational Sequence Entries/s]\n",
      "  0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning TvyZBvOMOTc:   0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/17 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning VCslbP0mgZI:   0%|          | 0/17 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/55 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning VbQk4H8hgr0:   0%|          | 0/55 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/9 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning Vj1wYRQjB-o:   0%|          | 0/9 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                  \u001B[A\n",
      "  0%|          | 0/32 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning W8NXH0Djyww:   0%|          | 0/32 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning WKA5OygbEKI:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/11 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning X3j2zQgwYgE:   0%|          | 0/11 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/9 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning ZAIRrfG22O0:   0%|          | 0/9 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                  \u001B[A\n",
      "  0%|          | 0/34 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning ZUXBRvtny7o:   0%|          | 0/34 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  60%|██████    | 56/93 [00:00<00:00, 78.04 Computational Sequence Entries/s]\n",
      "  0%|          | 0/28 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning _dI--eQ6qVU:   0%|          | 0/28 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning aiEXnCPZubE:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/21 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning atnd_PF-Lbs:   0%|          | 0/21 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/34 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning bOL9jKpeJRs:   0%|          | 0/34 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning bvLlb-M3UXU:   0%|          | 0/25 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning c5xsKMxpXnc:   0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/33 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning c7UH_rxdZv4:   0%|          | 0/33 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning cM3Yna7AavY:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  69%|██████▉   | 64/93 [00:00<00:00, 76.90 Computational Sequence Entries/s]\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning cW1FSBF59ik:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/29 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning cXypl4FnoZo:   0%|          | 0/29 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/19 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning d3_k5Xpfmik:   0%|          | 0/19 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/43 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning d6hH302o4v8:   0%|          | 0/43 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning dq3Nf_lMPnE:   0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/19 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning etzxEpPuc6I:   0%|          | 0/19 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning f9O3YtZ2VfI:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning f_pcplsH_V0:   0%|          | 0/15 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  77%|███████▋  | 72/93 [00:00<00:00, 77.05 Computational Sequence Entries/s]\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning fvVhgmXxadc:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning iiK8YX8oH1E:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/27 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning jUzDDGyPkXU:   0%|          | 0/27 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning k5Y_838nuGo:   0%|          | 0/31 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning lXPQBPVc5Cw:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/10 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning nbWiPyCm4g0:   0%|          | 0/10 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning nzpVDcQ0ywM:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning ob23OKe5a9Q:   0%|          | 0/14 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  86%|████████▌ | 80/93 [00:01<00:00, 77.79 Computational Sequence Entries/s]\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning pLTX3ipuDJI:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/21 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning phBUpBr1hSo:   0%|          | 0/21 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning rnaNMUZpvvg:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning tIrG4oNLFzE:   0%|          | 0/18 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning tStelxIAHjw:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/20 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning tmZoasNr4rU:   0%|          | 0/20 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning v0zCBqDeKcE:   0%|          | 0/16 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning vvZ4IcEtiZc:   0%|          | 0/12 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning vyB00TXsimI:   0%|          | 0/22 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning wMbj6ajWbic:   0%|          | 0/30 [00:00<?, ? Segments/s]\u001B[A\n",
      "Overall Progress:  97%|█████████▋| 90/93 [00:01<00:00, 82.69 Computational Sequence Entries/s]\n",
      "  0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning yDtzw_Y-7RU:   0%|          | 0/24 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/23 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning yvsjCA6Y5Fc:   0%|          | 0/23 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                   \u001B[A\n",
      "  0%|          | 0/35 [00:00<?, ? Segments/s]\u001B[A\n",
      "Aligning zhpQhgha_KU:   0%|          | 0/35 [00:00<?, ? Segments/s]\u001B[A\n",
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92m\u001B[1m[2022-04-02 23:12:16.463] | Success | \u001B[0mAlignment to <CMU_MOSI_Opinion_Labels> complete.\n",
      "\u001B[94m\u001B[1m[2022-04-02 23:12:16.463] | Status  | \u001B[0mReplacing dataset content with aligned computational sequences\n",
      "\u001B[92m\u001B[1m[2022-04-02 23:12:16.463] | Success | \u001B[0mInitialized empty <CMU_MOSI_Opinion_Labels> computational sequence.\n",
      "\u001B[94m\u001B[1m[2022-04-02 23:12:16.463] | Status  | \u001B[0mChecking the format of the data in <CMU_MOSI_Opinion_Labels> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/PycharmProjects/InterpretableMultimodal/utilss.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['file'][ind] = actual_file_name\n",
      "/home/dstratton/PycharmProjects/InterpretableMultimodal/utilss.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['audio'][ind] = {'array': s, 'sampling_rate': r, 'path': actual_file_name}\n",
      "/home/dstratton/PycharmProjects/InterpretableMultimodal/utilss.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['duration'][ind] = len(s) / r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92m\u001B[1m[2022-04-02 23:12:16.466] | Success | \u001B[0m<CMU_MOSI_Opinion_Labels> computational sequence data in correct format.\n",
      "\u001B[94m\u001B[1m[2022-04-02 23:12:16.466] | Status  | \u001B[0mChecking the format of the metadata in <CMU_MOSI_Opinion_Labels> computational sequence ...\n",
      "\u001B[93m\u001B[1m[2022-04-02 23:12:16.466] | Warning | \u001B[0m<CMU_MOSI_Opinion_Labels> computational sequence does not have all the required metadata ... continuing \n"
     ]
    }
   ],
   "source": [
    "# loading mosi into datasets dict\n",
    "# https://huggingface.co/blog/fine-tune-wav2vec2-english\n",
    "from utilss import load_mosi\n",
    "data = load_mosi([], load_audio=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/datasets/audio_process.html\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"labels\"] = batch[\"labels\"]\n",
    "\n",
    "    return batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1233 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4822356309b6419da193e48d351d89a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/216 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9604a98e23f47c2a4917e963e89cd80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/633 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70d47fe5c1a84e7f9265a3bae4c3918f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols_to_remove = data.column_names['train']\n",
    "cols_to_remove.remove('labels') # don't remove labels\n",
    "data = data.map(prepare_dataset,\n",
    "                remove_columns=cols_to_remove,\n",
    "                # num_proc=4\n",
    "                )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1283 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e7e6e2ca75b45ec8c6115a882ef8124"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/229 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09970c22108743d2bee8db9664c4fdd0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/686 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc9ee0b0bd884d53a67992cda055f2ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is a thing in pytorch, but not on the GPU!!!\n",
    "# inputs = feature_extractor(data[\"train\"][0][\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_values[0].cuda()\n",
    "inputs = feature_extractor(data[\"train\"][0][\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "inputs.data['input_values'] = inputs.data['input_values'].cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "model(**inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mderekdstratton\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=multimodal-emotion-rec\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "%env WANDB_PROJECT=multimodal-emotion-rec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "# optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "# lr_scheduler = AdafactorSchedule(optimizer)\n",
    "\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# TODO: maybe i need to resample the audio or do something with the\n",
    "# model? batch size of 1 is slightly less than ideal. 35 min 3 epoch\n",
    "# 20 min for 3 epochs on batch of 2. 4 is error i think\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"mosi-audio-model\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=2,\n",
    "  per_device_eval_batch_size=2,\n",
    "  report_to=\"wandb\",\n",
    "  num_train_epochs=20,\n",
    "  learning_rate=1e-7,\n",
    "  # learning_rate=None,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    # optimizers=(optimizer, lr_scheduler),\n",
    "    # data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"val\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    # tokenizer=processor\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1233\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6180\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/derekdstratton/multimodal-emotion-rec/runs/1za003l7\" target=\"_blank\">mosi-audio-model</a></strong> to <a href=\"https://wandb.ai/derekdstratton/multimodal-emotion-rec\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='6180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/6180 : < :, Epoch 0.00/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-309\n",
      "Configuration saved in mosi-audio-model/checkpoint-309/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-309/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-309/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-618\n",
      "Configuration saved in mosi-audio-model/checkpoint-618/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-618/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-618/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-927\n",
      "Configuration saved in mosi-audio-model/checkpoint-927/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-927/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-927/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-1236\n",
      "Configuration saved in mosi-audio-model/checkpoint-1236/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-1236/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-1236/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-1545\n",
      "Configuration saved in mosi-audio-model/checkpoint-1545/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-1545/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-1545/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-1854\n",
      "Configuration saved in mosi-audio-model/checkpoint-1854/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-1854/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-1854/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-2163\n",
      "Configuration saved in mosi-audio-model/checkpoint-2163/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-2163/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-2163/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-2472\n",
      "Configuration saved in mosi-audio-model/checkpoint-2472/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-2472/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-2472/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-2781\n",
      "Configuration saved in mosi-audio-model/checkpoint-2781/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-2781/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-2781/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-3090\n",
      "Configuration saved in mosi-audio-model/checkpoint-3090/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-3090/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-3090/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-3399\n",
      "Configuration saved in mosi-audio-model/checkpoint-3399/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-3399/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-3399/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-3708\n",
      "Configuration saved in mosi-audio-model/checkpoint-3708/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-3708/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-3708/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-4017\n",
      "Configuration saved in mosi-audio-model/checkpoint-4017/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-4017/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-4017/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-4326\n",
      "Configuration saved in mosi-audio-model/checkpoint-4326/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-4326/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-4326/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-4635\n",
      "Configuration saved in mosi-audio-model/checkpoint-4635/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-4635/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-4635/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-4944\n",
      "Configuration saved in mosi-audio-model/checkpoint-4944/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-4944/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-4944/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-5253\n",
      "Configuration saved in mosi-audio-model/checkpoint-5253/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-5253/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-5253/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-5562\n",
      "Configuration saved in mosi-audio-model/checkpoint-5562/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-5562/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-5562/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-5871\n",
      "Configuration saved in mosi-audio-model/checkpoint-5871/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-5871/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-5871/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 216\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to mosi-audio-model/checkpoint-6180\n",
      "Configuration saved in mosi-audio-model/checkpoint-6180/config.json\n",
      "Model weights saved in mosi-audio-model/checkpoint-6180/pytorch_model.bin\n",
      "Configuration saved in mosi-audio-model/checkpoint-6180/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 25889... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e588c460d55c4883b61d364d6fe6a854"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>▁▂▃█▃▅▃▅▅▃▅▅▅▅▃▅▅▅▅▅</td></tr><tr><td>eval/runtime</td><td>▃▄█▃▁▅▄▃▂▅▅█▃▆▄▃▇▁▅▄</td></tr><tr><td>eval/samples_per_second</td><td>▆▅▁▆█▄▅▆▇▄▄▁▆▃▅▆▂█▃▅</td></tr><tr><td>eval/steps_per_second</td><td>▆▅▁▆█▄▅▆▇▄▄▁▆▃▅▆▂█▃▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▅▄▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>▃█▁▆▅▇▃▄▅▂▅▄</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.54167</td></tr><tr><td>eval/loss</td><td>0.6897</td></tr><tr><td>eval/runtime</td><td>7.3817</td></tr><tr><td>eval/samples_per_second</td><td>29.262</td></tr><tr><td>eval/steps_per_second</td><td>7.315</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>6180</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6905</td></tr><tr><td>train/total_flos</td><td>8.140522821923923e+17</td></tr><tr><td>train/train_loss</td><td>0.69087</td></tr><tr><td>train/train_runtime</td><td>1640.134</td></tr><tr><td>train/train_samples_per_second</td><td>15.035</td></tr><tr><td>train/train_steps_per_second</td><td>3.768</td></tr></table>\n</div></div>\nSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">mosi-audio-model</strong>: <a href=\"https://wandb.ai/derekdstratton/multimodal-emotion-rec/runs/1za003l7\" target=\"_blank\">https://wandb.ai/derekdstratton/multimodal-emotion-rec/runs/1za003l7</a><br/>\nFind logs at: <code>./wandb/run-20220402_173045-1za003l7/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 4954... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c047e2ecd10f495883562fad7798d980"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>▂▅▂█▃█▁</td></tr><tr><td>eval/runtime</td><td>▅▅█▇▃▁▄</td></tr><tr><td>eval/samples_per_second</td><td>▄▄▁▂▆█▅</td></tr><tr><td>eval/steps_per_second</td><td>▄▄▁▂▆█▅</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▅▁▄</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.54167</td></tr><tr><td>eval/loss</td><td>0.68972</td></tr><tr><td>eval/runtime</td><td>7.4078</td></tr><tr><td>eval/samples_per_second</td><td>29.158</td></tr><tr><td>eval/steps_per_second</td><td>7.29</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>2163</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>0.693</td></tr></table>\n</div></div>\nSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">mosi-audio-model</strong>: <a href=\"https://wandb.ai/derekdstratton/multimodal-emotion-rec/runs/12btvnj6\" target=\"_blank\">https://wandb.ai/derekdstratton/multimodal-emotion-rec/runs/12btvnj6</a><br/>\nFind logs at: <code>./wandb/run-20220402_161413-12btvnj6/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1283\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1926\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mderekdstratton\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/derekdstratton/huggingface/runs/3d4kl5vt\" target=\"_blank\">audio-output</a></strong> to <a href=\"https://wandb.ai/derekdstratton/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1926' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1926 : < :, Epoch 0.00/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 229\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 229\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 229\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 229\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 229\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 229\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_186567/2254120986.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mtrain_results\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mwandb\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfinish\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    # optimizers=(optimizer, lr_scheduler),\n",
    "    # data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"val\"],\n",
    "    # tokenizer=feature_extractor,\n",
    "    tokenizer=processor\n",
    ")\n",
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 633\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/159 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'test_loss': 0.7083980441093445,\n 'test_accuracy': 0.4186413902053712,\n 'test_runtime': 26.3797,\n 'test_samples_per_second': 23.996,\n 'test_steps_per_second': 6.027}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(data[\"test\"])\n",
    "\n",
    "# test_accuracy is correct, predictions for each class, you can take argmax\n",
    "predictions.metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### below here playing around with things"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_1.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_2.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_3.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_4.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_5.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_6.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_7.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_8.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_9.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_10.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_11.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_12.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/03bSnISJMiM_13.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_1.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_2.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_3.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_4.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_5.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_6.wav',\n 'cmumosi/Raw/Audio/WAV_16000/Segmented/0h-zjBukYpk_7.wav']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['file'][:20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from transformers import TrainerCallback\n",
    "# make a callback to record training loss\n",
    "# https://stackoverflow.com/questions/67457480/how-to-get-the-accuracy-per-epoch-or-step-for-the-huggingface-transformers-train\n",
    "# https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/callback\n",
    "class CustomCallback(TrainerCallback):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # self._trainer = trainer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = deepcopy(control)\n",
    "            trainer.evaluate(eval_dataset=trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            # return control\n",
    "\n",
    "            # self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy\n",
    "\n",
    "# trainer.add_callback(CustomCallback())\n",
    "# trainer.add_callback(CustomCallback(trainer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 686\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='172' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/172 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'test_loss': 0.7078453898429871,\n 'test_accuracy': 0.4037900874635568,\n 'test_runtime': 28.0898,\n 'test_samples_per_second': 24.422,\n 'test_steps_per_second': 6.123}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(data[\"test\"])\n",
    "\n",
    "# test_accuracy is correct, predictions for each class, you can take argmax\n",
    "predictions.metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# data collator (https://huggingface.co/blog/fine-tune-wav2vec2-english)\n",
    "# todo: learn what this is! LOL\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "# data_collator = DataCollatorCTCWithPadding(processor=feature_extractor, padding=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "1    678\n0    605\ndtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_labels = pd.DataFrame(data['train']['labels'])\n",
    "train_labels.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "1    124\n0    105\ndtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels = pd.DataFrame(data['val']['labels'])\n",
    "val_labels.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "0    409\n1    277\ndtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = pd.DataFrame(data['test']['labels'])\n",
    "test_labels.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_features = pd.DataFrame(data['train']['input_values'][:5])\n",
    "# this ends up being like 5 rows, 160K cols lol"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import shap\n",
    "# explainer = shap.Explainer(pred, processor)\n",
    "# way too slow. any way of looking at it by segmenting off words?\n",
    "# shap_values = explainer([speech])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}