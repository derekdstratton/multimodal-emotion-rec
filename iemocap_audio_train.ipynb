{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4454/587072343.py:2: FutureWarning: Could not cast to float32, falling back to object. This behavior is deprecated. In a future version, when a dtype is passed to 'DataFrame', either all columns will be cast to that dtype, or a TypeError will be raised\n",
      "  iemocap_dataset = IemocapDataset('/home/dstratton/IEMOCAP_full_release')\n"
     ]
    }
   ],
   "source": [
    "from torchemotion.datasets.IemocapDataset import *\n",
    "iemocap_dataset = IemocapDataset('/home/dstratton/IEMOCAP_full_release')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAARVUlEQVR4nO3db8id9X3H8fdn0VlpK9V5K1mSLa5kMJUtnSETOoZbt5rVB7EDR3xQMyikiIUW9qCxT9oOAm6s3RBWIaViHF0l0HaGtW7LpKMruNpbSRtj6gw10zQhudtSqk8cpt89OL9sZ7fn/p+c+9z7vV9wONf5nuvP9/wwn1z+znWupKqQJPXh51a7AUnS+Bj6ktQRQ1+SOmLoS1JHDH1J6shlq93AQq699travHnzarchSWvKM88888Oqmppdn/jQ37x5M9PT06vdhiStKUn+c1Td6R1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerIxP8idyU27/3qqhz35AN3rMpxJWkhnulLUkcWDP0kb0nydJLvJDmW5FOtfk2Sw0lebM9XD21zf5ITSV5IcvtQ/ZYkR9t7DybJpflYkqRRFnOm/zrwe1X1G8BWYEeSW4G9wJNVtQV4sr0myY3ALuAmYAfw2STr2r4eAvYAW9pjx8X7KJKkhSwY+jXwWnt5eXsUsBM40OoHgDvb8k7gsap6vapeAk4A25OsB66qqqdq8K+xPzq0jSRpDBY1p59kXZIjwDngcFV9C7i+qs4AtOfr2uobgFeGNj/Vahva8uz6qOPtSTKdZHpmZmYJH0eSNJ9FhX5Vna+qrcBGBmftN8+z+qh5+pqnPup4+6tqW1Vtm5p6078BIElapiVdvVNVPwH+lcFc/Nk2ZUN7PtdWOwVsGtpsI3C61TeOqEuSxmQxV+9MJXlHW74S+H3ge8AhYHdbbTfweFs+BOxKckWSGxh8Yft0mwJ6Ncmt7aqde4a2kSSNwWJ+nLUeONCuwPk54GBV/UOSp4CDST4IvAzcBVBVx5IcBJ4H3gDuq6rzbV/3Ao8AVwJPtIckaUwWDP2q+i7wrhH1HwHvmWObfcC+EfVpYL7vAyRJl5C/yJWkjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0JekjiwY+kk2Jfl6kuNJjiX5SKt/MskPkhxpj/cNbXN/khNJXkhy+1D9liRH23sPJsml+ViSpFEuW8Q6bwB/WlXPJnk78EySw+29v6qqvxxeOcmNwC7gJuAXgX9J8qtVdR54CNgD/DvwNWAH8MTF+SiSpIUseKZfVWeq6tm2/CpwHNgwzyY7gceq6vWqegk4AWxPsh64qqqeqqoCHgXuXOkHkCQt3pLm9JNsBt4FfKuVPpzku0keTnJ1q20AXhna7FSrbWjLs+ujjrMnyXSS6ZmZmaW0KEmax6JDP8nbgC8BH62qnzKYqnknsBU4A3z6wqojNq956m8uVu2vqm1VtW1qamqxLUqSFrCo0E9yOYPA/0JVfRmgqs5W1fmq+hnwOWB7W/0UsGlo843A6VbfOKIuSRqTxVy9E+DzwPGq+sxQff3Qau8HnmvLh4BdSa5IcgOwBXi6qs4Arya5te3zHuDxi/Q5JEmLsJird94NfAA4muRIq30cuDvJVgZTNCeBDwFU1bEkB4HnGVz5c1+7cgfgXuAR4EoGV+145Y4kjdGCoV9V32T0fPzX5tlmH7BvRH0auHkpDUqSLh5/kStJHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerIgqGfZFOSryc5nuRYko+0+jVJDid5sT1fPbTN/UlOJHkhye1D9VuSHG3vPZgkl+ZjSZJGWcyZ/hvAn1bVrwG3AvcluRHYCzxZVVuAJ9tr2nu7gJuAHcBnk6xr+3oI2ANsaY8dF/GzSJIWsGDoV9WZqnq2Lb8KHAc2ADuBA221A8CdbXkn8FhVvV5VLwEngO1J1gNXVdVTVVXAo0PbSJLGYElz+kk2A+8CvgVcX1VnYPAXA3BdW20D8MrQZqdabUNbnl0fdZw9SaaTTM/MzCylRUnSPBYd+kneBnwJ+GhV/XS+VUfUap76m4tV+6tqW1Vtm5qaWmyLkqQFLCr0k1zOIPC/UFVfbuWzbcqG9nyu1U8Bm4Y23wicbvWNI+qSpDFZzNU7AT4PHK+qzwy9dQjY3ZZ3A48P1XcluSLJDQy+sH26TQG9muTWts97hraRJI3BZYtY593AB4CjSY602seBB4CDST4IvAzcBVBVx5IcBJ5ncOXPfVV1vm13L/AIcCXwRHtIksZkwdCvqm8yej4e4D1zbLMP2DeiPg3cvJQGJUkXj7/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerIZavdwP9Hm/d+ddWOffKBO1bt2JImn2f6ktQRQ1+SOmLoS1JHFgz9JA8nOZfkuaHaJ5P8IMmR9njf0Hv3JzmR5IUktw/Vb0lytL33YJJc/I8jSZrPYs70HwF2jKj/VVVtbY+vASS5EdgF3NS2+WySdW39h4A9wJb2GLVPSdIltGDoV9U3gB8vcn87gceq6vWqegk4AWxPsh64qqqeqqoCHgXuXGbPkqRlWsmc/oeTfLdN/1zdahuAV4bWOdVqG9ry7PpISfYkmU4yPTMzs4IWJUnDlhv6DwHvBLYCZ4BPt/qoefqapz5SVe2vqm1VtW1qamqZLUqSZltW6FfV2ao6X1U/Az4HbG9vnQI2Da26ETjd6htH1CVJY7Ss0G9z9Be8H7hwZc8hYFeSK5LcwOAL26er6gzwapJb21U79wCPr6BvSdIyLHgbhiRfBG4Drk1yCvgEcFuSrQymaE4CHwKoqmNJDgLPA28A91XV+barexlcCXQl8ER7SJLGaMHQr6q7R5Q/P8/6+4B9I+rTwM1L6k6SdFH5i1xJ6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHFgz9JA8nOZfkuaHaNUkOJ3mxPV899N79SU4keSHJ7UP1W5Icbe89mCQX/+NIkuazmDP9R4Ads2p7gSeragvwZHtNkhuBXcBNbZvPJlnXtnkI2ANsaY/Z+5QkXWILhn5VfQP48azyTuBAWz4A3DlUf6yqXq+ql4ATwPYk64Grquqpqirg0aFtJEljstw5/eur6gxAe76u1TcArwytd6rVNrTl2XVJ0hhd7C9yR83T1zz10TtJ9iSZTjI9MzNz0ZqTpN4tN/TPtikb2vO5Vj8FbBpabyNwutU3jqiPVFX7q2pbVW2bmppaZouSpNmWG/qHgN1teTfw+FB9V5IrktzA4Avbp9sU0KtJbm1X7dwztI0kaUwuW2iFJF8EbgOuTXIK+ATwAHAwyQeBl4G7AKrqWJKDwPPAG8B9VXW+7epeBlcCXQk80R6SpDFaMPSr6u453nrPHOvvA/aNqE8DNy+pO0nSReUvciWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpIysK/SQnkxxNciTJdKtdk+Rwkhfb89VD69+f5ESSF5LcvtLmJUlLc9lF2MfvVtUPh17vBZ6sqgeS7G2vP5bkRmAXcBPwi8C/JPnVqjp/EXpQs3nvV1fluCcfuGNVjitpaS7F9M5O4EBbPgDcOVR/rKper6qXgBPA9ktwfEnSHFYa+gX8c5Jnkuxpteur6gxAe76u1TcArwxte6rV3iTJniTTSaZnZmZW2KIk6YKVTu+8u6pOJ7kOOJzke/OsmxG1GrViVe0H9gNs27Zt5DqSpKVb0Zl+VZ1uz+eArzCYrjmbZD1Aez7XVj8FbBrafCNweiXHlyQtzbJDP8lbk7z9wjLwXuA54BCwu622G3i8LR8CdiW5IskNwBbg6eUeX5K0dCuZ3rke+EqSC/v5u6r6xyTfBg4m+SDwMnAXQFUdS3IQeB54A7jPK3ckabyWHfpV9X3gN0bUfwS8Z45t9gH7lntMSdLK+ItcSeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOrLSfyNXAmDz3q+u2rFPPnDHqh1bWms805ekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kd8ZJNrXmrdbmol4pqLfJMX5I6YuhLUkfGHvpJdiR5IcmJJHvHfXxJ6tlYQz/JOuBvgD8EbgTuTnLjOHuQpJ6N+4vc7cCJqvo+QJLHgJ3A82PuQ1qx1bzf0Grxy+u1b9yhvwF4Zej1KeC3Zq+UZA+wp718LckLi9j3tcAPV9zheNnz+KzFvieu5/z5gqtMXM+LtBb7XqjnXx5VHHfoZ0St3lSo2g/sX9KOk+mq2rbcxlaDPY/PWuzbnsdnLfa93J7H/UXuKWDT0OuNwOkx9yBJ3Rp36H8b2JLkhiQ/D+wCDo25B0nq1lind6rqjSQfBv4JWAc8XFXHLtLulzQdNCHseXzWYt/2PD5rse9l9ZyqN02pS5L+n/IXuZLUEUNfkjqy5kN/rd7WIcnJJEeTHEkyvdr9jJLk4STnkjw3VLsmyeEkL7bnq1ezx9nm6PmTSX7QxvpIkvetZo+zJdmU5OtJjic5luQjrT7pYz1X3xM73knekuTpJN9pPX+q1Sd2rOfpeVnjvKbn9NttHf4D+AMGl4N+G7i7qib+F75JTgLbqmpifxCS5HeA14BHq+rmVvsL4MdV9UD7S/bqqvrYavY5bI6ePwm8VlV/uZq9zSXJemB9VT2b5O3AM8CdwJ8w2WM9V99/zISOd5IAb62q15JcDnwT+AjwR0zoWM/T8w6WMc5r/Uz/f27rUFX/BVy4rYMugqr6BvDjWeWdwIG2fIDBH/KJMUfPE62qzlTVs235VeA4g1+vT/pYz9X3xKqB19rLy9ujmOCxnqfnZVnroT/qtg4T/R/dkAL+Ockz7bYTa8X1VXUGBn/ogetWuZ/F+nCS77bpn4n5X/fZkmwG3gV8izU01rP6hgke7yTrkhwBzgGHq2rix3qOnmEZ47zWQ39Rt3WYUO+uqt9kcMfR+9q0hC6Nh4B3AluBM8CnV7WbOSR5G/Al4KNV9dPV7mexRvQ90eNdVeeraiuDOwJsT3LzKre0oDl6XtY4r/XQX7O3daiq0+35HPAVBlNVa8HZNpd7YU733Cr3s6CqOtv+0PwM+BwTONZtrvZLwBeq6sutPPFjParvtTDeAFX1E+BfGcyNT/xYw//tebnjvNZDf03e1iHJW9sXXyR5K/Be4Ln5t5oYh4DdbXk38Pgq9rIoF/4wN+9nwsa6fVH3eeB4VX1m6K2JHuu5+p7k8U4yleQdbflK4PeB7zHBYz1Xz8sd5zV99Q5Au0zpr/nf2zrsW92OFpbkVxic3cPgVhh/N4l9J/kicBuDW7ieBT4B/D1wEPgl4GXgrqqamC9O5+j5Ngb/C1zASeBDF+ZvJ0GS3wb+DTgK/KyVP85gfnySx3quvu9mQsc7ya8z+KJ2HYOT3oNV9WdJfoEJHet5ev5bljHOaz70JUmLt9andyRJS2DoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI78NwREhnBgaR3oAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(iemocap_dataset.df[\"end\"] - iemocap_dataset.df[\"start\"])\n",
    "print(sum(iemocap_dataset.df[\"end\"] - iemocap_dataset.df[\"start\"] > 10))\n",
    "# remove super long clips\n",
    "iemocap_dataset.df = iemocap_dataset.df[iemocap_dataset.df[\"end\"] - iemocap_dataset.df[\"start\"] < 10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# todo: map labels to be in range [0, 4), since they aren't right now. maybe thats why nll loss is sad\n",
    "mapping = dict([(y,x) for x,y in enumerate(sorted(set(iemocap_dataset.df[\"emotion\"] )))])\n",
    "iemocap_dataset.df[\"emotion\"] = iemocap_dataset.df[\"emotion\"].apply(lambda x: mapping[x])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# splitting into sets\n",
    "import numpy as np, torch\n",
    "# https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset\n",
    "# todo: i could do this if i can rand seed it torch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>)\n",
    "np.random.seed(0)\n",
    "indices = np.arange(0, len(iemocap_dataset))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:int(len(iemocap_dataset)*.8)]\n",
    "val_indices = indices[int(len(iemocap_dataset)*.8):int(len(iemocap_dataset)*.9)]\n",
    "test_indices = indices[int(len(iemocap_dataset)*.9):]\n",
    "# train_set = torch.utils.data.Subset(iemocap_dataset, train_indices)\n",
    "# val_set = torch.utils.data.Subset(iemocap_dataset, val_indices)\n",
    "# test_set = torch.utils.data.Subset(iemocap_dataset, test_indices)\n",
    "# todo: may be good to balance or at least check the balance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4454/2646199776.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['audio'][ind] = {'array': s, 'sampling_rate': r}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import datasets\n",
    "\n",
    "data_dict = {}\n",
    "for split_name, indices in {\"train\": train_indices, \"val\": val_indices, \"test\": test_indices}.items():\n",
    "    ind = 0\n",
    "    df = pd.DataFrame(iemocap_dataset.df[\"emotion\"].iloc[indices]).reset_index()\n",
    "    # text\n",
    "    # df['text'] = iemocap_dataset.df[\"transcription\"]\n",
    "    # audio\n",
    "    df['audio'] = None\n",
    "    for file_name in iemocap_dataset.df[\"file\"].iloc[indices]:\n",
    "        actual_file_name = '/home/dstratton/IEMOCAP_full_release/' + file_name\n",
    "        s, r = librosa.load(actual_file_name, sr=16000)\n",
    "        df['audio'][ind] = {'array': s, 'sampling_rate': r}\n",
    "        ind += 1\n",
    "    data_dict[split_name] = datasets.Dataset.from_pandas(df)\n",
    "\n",
    "data = datasets.DatasetDict(data_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_q.bias', 'project_hid.weight', 'quantizer.weight_proj.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'projector.weight', 'projector.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# audio\n",
    "# base models\n",
    "import transformers\n",
    "# https://huggingface.co/facebook/wav2vec2-base\n",
    "# 16 kHz sampling rate in original\n",
    "processor = transformers.Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\", use_fast=True)\n",
    "model = transformers.AutoModelForAudioClassification.from_pretrained(\"facebook/wav2vec2-base\", num_labels=4).cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Wav2Vec2FeatureExtractor {\n  \"do_normalize\": true,\n  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n  \"feature_size\": 1,\n  \"padding_side\": \"right\",\n  \"padding_value\": 0.0,\n  \"return_attention_mask\": false,\n  \"sampling_rate\": 16000\n}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "# https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/audio_classification.ipynb#scrollTo=G1bX4lGAO_d9\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "feature_extractor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/datasets/audio_process.html\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"labels\"] = int(batch[\"emotion\"])\n",
    "    return batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4129 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9abf15d350fb4d1fbf9d74c8595ebd64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/516 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "303e143cbfe445a6aeb81d96e59c7312"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/517 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f8e16e8f8384ea198c355269ae17a6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.map(prepare_dataset,\n",
    "                batch_size=1,\n",
    "                remove_columns=['index'],\n",
    "                # num_proc=4\n",
    "                )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mderekdstratton\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=multimodal-emotion-rec\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "%env WANDB_PROJECT=multimodal-emotion-rec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"iemocap-audio-model\",\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    report_to=\"wandb\",\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=1e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    # data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['val'],\n",
    "    tokenizer=feature_extractor\n",
    "\n",
    "    # tokenizer=processor,\n",
    "    # tokenizer=tokenizer\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running training *****\n",
      "  Num examples = 4129\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20660\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/derekdstratton/multimodal-emotion-rec/runs/4iyytb5n\" target=\"_blank\">iemocap-audio-model</a></strong> to <a href=\"https://wandb.ai/derekdstratton/multimodal-emotion-rec\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/transformers/feature_extraction_utils.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180487213/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  tensor = as_tensor(value)\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='20660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/20660 : < :, Epoch 0.00/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-1033\n",
      "Configuration saved in iemocap-audio-model/checkpoint-1033/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-1033/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-1033/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-2066\n",
      "Configuration saved in iemocap-audio-model/checkpoint-2066/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-2066/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-2066/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-3099\n",
      "Configuration saved in iemocap-audio-model/checkpoint-3099/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-3099/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-3099/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-4132\n",
      "Configuration saved in iemocap-audio-model/checkpoint-4132/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-4132/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-4132/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-5165\n",
      "Configuration saved in iemocap-audio-model/checkpoint-5165/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-5165/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-5165/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-6198\n",
      "Configuration saved in iemocap-audio-model/checkpoint-6198/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-6198/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-6198/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-7231\n",
      "Configuration saved in iemocap-audio-model/checkpoint-7231/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-7231/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-7231/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-8264\n",
      "Configuration saved in iemocap-audio-model/checkpoint-8264/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-8264/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-8264/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-9297\n",
      "Configuration saved in iemocap-audio-model/checkpoint-9297/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-9297/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-9297/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-10330\n",
      "Configuration saved in iemocap-audio-model/checkpoint-10330/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-10330/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-10330/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-11363\n",
      "Configuration saved in iemocap-audio-model/checkpoint-11363/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-11363/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-11363/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-12396\n",
      "Configuration saved in iemocap-audio-model/checkpoint-12396/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-12396/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-12396/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-13429\n",
      "Configuration saved in iemocap-audio-model/checkpoint-13429/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-13429/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-13429/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-14462\n",
      "Configuration saved in iemocap-audio-model/checkpoint-14462/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-14462/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-14462/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-15495\n",
      "Configuration saved in iemocap-audio-model/checkpoint-15495/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-15495/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-15495/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-16528\n",
      "Configuration saved in iemocap-audio-model/checkpoint-16528/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-16528/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-16528/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-17561\n",
      "Configuration saved in iemocap-audio-model/checkpoint-17561/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-17561/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-17561/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-18594\n",
      "Configuration saved in iemocap-audio-model/checkpoint-18594/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-18594/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-18594/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-19627\n",
      "Configuration saved in iemocap-audio-model/checkpoint-19627/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-19627/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-19627/preprocessor_config.json\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to iemocap-audio-model/checkpoint-20660\n",
      "Configuration saved in iemocap-audio-model/checkpoint-20660/config.json\n",
      "Model weights saved in iemocap-audio-model/checkpoint-20660/pytorch_model.bin\n",
      "Configuration saved in iemocap-audio-model/checkpoint-20660/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 96995... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c723818ddb24e5bba55df3987fd1aed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▃▂▄▄▇▆█▆▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>eval/loss</td><td>▁▂▁▃▂▄▂▄▃▆▅▅▆▇▇▇████</td></tr><tr><td>eval/runtime</td><td>▁▃▃▄▄▅▆▆▅▆▆▇▅▅▆▆█▆▇█</td></tr><tr><td>eval/samples_per_second</td><td>█▆▆▅▅▄▃▃▄▃▃▂▄▄▃▃▁▃▂▁</td></tr><tr><td>eval/steps_per_second</td><td>█▆▆▅▅▄▃▃▄▃▃▂▄▄▃▃▁▃▂▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.72093</td></tr><tr><td>eval/loss</td><td>2.19329</td></tr><tr><td>eval/runtime</td><td>19.506</td></tr><tr><td>eval/samples_per_second</td><td>26.453</td></tr><tr><td>eval/steps_per_second</td><td>6.613</td></tr><tr><td>train/epoch</td><td>20.0</td></tr><tr><td>train/global_step</td><td>20660</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0696</td></tr><tr><td>train/total_flos</td><td>2.9755301286116864e+18</td></tr><tr><td>train/train_loss</td><td>0.38789</td></tr><tr><td>train/train_runtime</td><td>5505.2334</td></tr><tr><td>train/train_samples_per_second</td><td>15.0</td></tr><tr><td>train/train_steps_per_second</td><td>3.753</td></tr></table>\n</div></div>\nSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">iemocap-audio-model</strong>: <a href=\"https://wandb.ai/derekdstratton/multimodal-emotion-rec/runs/4iyytb5n\" target=\"_blank\">https://wandb.ai/derekdstratton/multimodal-emotion-rec/runs/4iyytb5n</a><br/>\nFind logs at: <code>./wandb/run-20220402_130943-4iyytb5n/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: emotion, audio.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 517\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/130 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'test_loss': 2.1163454055786133,\n 'test_accuracy': 0.7330754352030948,\n 'test_runtime': 20.8287,\n 'test_samples_per_second': 24.822,\n 'test_steps_per_second': 6.241}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(data[\"test\"])\n",
    "\n",
    "# test_accuracy is correct, predictions for each class, you can take argmax\n",
    "predictions.metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: audio, emotion.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 516\n",
      "  Batch size = 4\n",
      "/home/dstratton/miniconda3/envs/InterpretableMultimodal/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/129 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 1.3443185091018677,\n 'eval_accuracy': 0.35658914728682173,\n 'eval_runtime': 20.5995,\n 'eval_samples_per_second': 25.049,\n 'eval_steps_per_second': 6.262}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}